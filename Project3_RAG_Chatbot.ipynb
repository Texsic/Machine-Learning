{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Texsic/Machine-Learning/blob/main/Project3_RAG_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgOe3NtB-q2X"
      },
      "source": [
        "# MLB Project 3 - RAG Chatbot\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "Welcome to Project 3! In this project, you'll build a **Retrieval-Augmented Generation (RAG)** chatbot that can answer questions about a PDF document.\n",
        "\n",
        "### What is RAG?\n",
        "RAG combines two powerful concepts:\n",
        "1. **Retrieval**: Finding relevant information from a document\n",
        "2. **Generation**: Using an LLM to generate natural language answers\n",
        "\n",
        "### What You'll Learn\n",
        "- How to extract and process text from PDFs\n",
        "- How to create text embeddings (vector representations)\n",
        "- How to build a simple vector database\n",
        "- How to search for relevant information using similarity\n",
        "- How to use an LLM to generate answers based on context\n",
        "\n",
        "### Project Structure\n",
        "1. Setup and imports\n",
        "2. Build a Vector Database\n",
        "3. PDF processing utilities\n",
        "4. Question answering system\n",
        "5. Put it all together!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkvDxZKt-q2Y"
      },
      "source": [
        "## Step 1: Setup and Imports\n",
        "\n",
        "First, let's install the required libraries and import them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "N_raVXQ2-q2Z"
      },
      "outputs": [],
      "source": [
        "# Install required packages (run this cell first!)\n",
        "!pip install sentence-transformers pypdf transformers huggingface_hub torch tqdm -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "w9Sk_S1i-q2Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7805cb3b-76e6-4db1-c5ef-dc4cdde8bc08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import all necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pypdf import PdfReader\n",
        "from transformers import pipeline\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Suppress unnecessary warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "print(\"âœ… All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGKilY6Y-q2Z"
      },
      "source": [
        "## Step 2: Configuration\n",
        "\n",
        "Let's set up our model names and API keys."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FaYaJfws-q2Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8c68204-f7d2-44dc-9cc8-a0b9c6438771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“‹ LLM Model: google/flan-t5-base\n",
            "ðŸ“‹ Embedding Model: all-MiniLM-L12-v2\n"
          ]
        }
      ],
      "source": [
        "# Configuration settings\n",
        "LLM_MODEL = \"google/flan-t5-base\"  # The language model for generating answers\n",
        "EMBEDDING_MODEL = \"all-MiniLM-L12-v2\"  # The model for creating embeddings\n",
        "HF_API_KEY = os.getenv(\"HF_API_KEY\", \"YOUR-HF-API-KEY-HERE\")  # Optional Hugging Face API key\n",
        "\n",
        "print(f\"ðŸ“‹ LLM Model: {LLM_MODEL}\")\n",
        "print(f\"ðŸ“‹ Embedding Model: {EMBEDDING_MODEL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqaXXQ84-q2a"
      },
      "source": [
        "## Step 3: Build the Vector Database Class\n",
        "\n",
        "A **Vector Database** stores text as numerical vectors (embeddings) and allows us to search for similar text using mathematical operations.\n",
        "\n",
        "### What are Embeddings?\n",
        "Embeddings are numerical representations of text that capture semantic meaning. Similar texts have similar embeddings.\n",
        "\n",
        "Example:\n",
        "- \"dog\" and \"puppy\" would have similar embeddings\n",
        "- \"dog\" and \"car\" would have very different embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYncA5dj-q2a"
      },
      "source": [
        "### 3.1: Initialize the Vector Database\n",
        "\n",
        "**TODO**: Complete the `__init__` method to:\n",
        "1. Load the SentenceTransformer model\n",
        "2. Get the embedding dimension\n",
        "3. Initialize empty storage for embeddings and text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "XGjPWOwp-q2a"
      },
      "outputs": [],
      "source": [
        "class VectorDB:\n",
        "    def __init__(self, model_name: str):\n",
        "        \"\"\"\n",
        "        Initialize the Vector Database with an embedding model.\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the SentenceTransformer model to use\n",
        "        \"\"\"\n",
        "        # TODO: Load the SentenceTransformer embedding model\n",
        "        # Hint: self.embedModel = SentenceTransformer(model_name)\n",
        "        self.embedModel = SentenceTransformer(model_name)\n",
        "\n",
        "        # TODO: Get the embedding dimension from the model\n",
        "        # Hint: Use get_sentence_embedding_dimension()\n",
        "        self.embed_size = self.embedModel.get_sentence_embedding_dimension()\n",
        "\n",
        "        # TODO: Initialize an empty NumPy array for embeddings\n",
        "        # Hint: Start with np.empty((0, self.embed_size))\n",
        "        self._embeddings = np.empty((0, self.embed_size))\n",
        "\n",
        "        # TODO: Initialize an empty list for storing the original text strings\n",
        "        self._strings = []\n",
        "\n",
        "        print(f\"âœ… VectorDB initialized with embedding dimension: {self.embed_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmjkXXTS-q2a"
      },
      "source": [
        "### 3.2: Add Data to the Database\n",
        "\n",
        "**TODO**: Complete the `addToDatabase` method to:\n",
        "1. Convert text strings to embeddings\n",
        "2. Store the embeddings in the database\n",
        "3. Store the original text strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "vTolhy7L-q2a"
      },
      "outputs": [],
      "source": [
        "    def addToDatabase(self, input: list[str]):\n",
        "        \"\"\"\n",
        "        Add text chunks to the vector database.\n",
        "\n",
        "        Args:\n",
        "            input: List of text strings to add to the database\n",
        "        \"\"\"\n",
        "        # TODO: Convert the input strings to embeddings\n",
        "        # Hint: Use self.embedModel.encode(input) to get embeddings\n",
        "        new_embeddings = self.embedModel.encode(input)\n",
        "\n",
        "        # TODO: Stack the new embeddings with existing ones\n",
        "        # Hint: Use np.vstack() to append vertically\n",
        "        # Handle the case where _embeddings is empty (first addition)\n",
        "        if self._embeddings.shape[0] == 0:\n",
        "            self._embeddings = new_embeddings\n",
        "        else:\n",
        "            self._embeddings = np.vstack((self._embeddings, new_embeddings))\n",
        "\n",
        "        # TODO: Extend the _strings list with the new input strings\n",
        "        # Hint: Use list.extend()\n",
        "        self._strings.extend(input)\n",
        "\n",
        "        print(f\"âœ… Added {len(input)} chunks. Total chunks: {len(self._strings)}\")\n",
        "\n",
        "# Add this method to the VectorDB class\n",
        "VectorDB.addToDatabase = addToDatabase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h2DF0Nc-q2a"
      },
      "source": [
        "### 3.3: Clear the Database\n",
        "\n",
        "**TODO**: Complete the `clearDatabase` method to reset the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "DUJIbgLu-q2a"
      },
      "outputs": [],
      "source": [
        "    def clearDatabase(self):\n",
        "        \"\"\"\n",
        "        Clear all data from the vector database.\n",
        "        \"\"\"\n",
        "        # TODO: Reset _embeddings to an empty array with the correct shape\n",
        "        # Hint: Use np.empty((0, self.embed_size))\n",
        "        self._embeddings = np.empty((0, self.embed_size))\n",
        "\n",
        "        # TODO: Reset _strings to an empty list\n",
        "        self._strings = []\n",
        "\n",
        "        print(\"ðŸ—‘ï¸ Database cleared!\")\n",
        "\n",
        "# Add this method to the VectorDB class\n",
        "VectorDB.clearDatabase = clearDatabase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G583uLg-q2a"
      },
      "source": [
        "### 3.4: Calculate Euclidean Similarity\n",
        "\n",
        "**TODO**: Implement the similarity function.\n",
        "\n",
        "**What is Euclidean Distance?**\n",
        "It's the straight-line distance between two points in space. We convert it to similarity:\n",
        "- Distance = 0 â†’ Similarity = 1 (identical)\n",
        "- Distance = large â†’ Similarity = close to 0 (very different)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qnrtKLNl-q2b"
      },
      "outputs": [],
      "source": [
        "    def euclideanSim(self, x, y):\n",
        "        \"\"\"\n",
        "        Calculate Euclidean similarity between two vectors.\n",
        "\n",
        "        Args:\n",
        "            x: First vector (numpy array)\n",
        "            y: Second vector (numpy array)\n",
        "\n",
        "        Returns:\n",
        "            Similarity score (higher = more similar)\n",
        "        \"\"\"\n",
        "        # TODO: Calculate Euclidean distance using np.linalg.norm()\n",
        "        # Hint: distance = np.linalg.norm(x - y)\n",
        "        distance = np.linalg.norm(x - y)\n",
        "\n",
        "        # TODO: Convert distance to similarity\n",
        "        # Hint: similarity = 1 / (1 + distance)\n",
        "        similarity = 1 / (1 + distance)\n",
        "\n",
        "        return similarity\n",
        "\n",
        "# Add this method to the VectorDB class\n",
        "VectorDB.euclideanSim = euclideanSim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os2LZVtW-q2b"
      },
      "source": [
        "### 3.5: Search the Database\n",
        "\n",
        "**TODO**: Implement the search functionality to find the most similar text chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "aZPv7W8Q-q2b"
      },
      "outputs": [],
      "source": [
        "    def search(self, query: str, n_return=3):\n",
        "        \"\"\"\n",
        "        Search the database for the most similar chunks to the query.\n",
        "\n",
        "        Args:\n",
        "            query: The search query string\n",
        "            n_return: Number of top results to return\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (text_chunks, similarity_scores)\n",
        "        \"\"\"\n",
        "        # TODO: Generate an embedding for the query\n",
        "        # Hint: Use self.embedModel.encode([query])[0] to get a single embedding\n",
        "        query_embedding = self.embedModel.encode([query])[0]\n",
        "\n",
        "        # TODO: Calculate similarity between query and all stored embeddings\n",
        "        # Hint: Use a list comprehension with self.euclideanSim()\n",
        "        similarities = [self.euclideanSim(query_embedding, emb) for emb in self._embeddings]\n",
        "        # Example: [self.euclideanSim(query_embedding, emb) for emb in self._embeddings]\n",
        "\n",
        "        # TODO: Find indices of the top n_return most similar results\n",
        "        # Hint: Use np.argsort() and reverse the order with [::-1]\n",
        "        top_indices = np.argsort(similarities)[::-1][:n_return]\n",
        "\n",
        "        # TODO: Get the corresponding text chunks and scores\n",
        "        top_chunks = [self._strings[i] for i in top_indices]\n",
        "        top_scores = [similarities[i] for i in top_indices]\n",
        "\n",
        "        return top_chunks, top_scores\n",
        "\n",
        "# Add this method to the VectorDB class\n",
        "VectorDB.search = search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXS0oe4a-q2b"
      },
      "source": [
        "### Test the Vector Database\n",
        "\n",
        "Let's test our VectorDB with some sample data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "3_BEoOsU-q2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb1f944-802e-4ddb-b789-f53f240cf4d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing VectorDB...\n",
            "\n",
            "âœ… VectorDB initialized with embedding dimension: 384\n",
            "âœ… Added 4 chunks. Total chunks: 4\n",
            "\n",
            "ðŸ” Query: 'What is ML?'\n",
            "\n",
            "Result 1 (similarity: 0.4539):\n",
            "  Machine learning involves training models on data.\n",
            "\n",
            "Result 2 (similarity: 0.4379):\n",
            "  Neural networks are inspired by the human brain.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test the VectorDB\n",
        "print(\"Testing VectorDB...\\n\")\n",
        "\n",
        "# Create a test database\n",
        "test_vdb = VectorDB(EMBEDDING_MODEL)\n",
        "\n",
        "# Add some test data\n",
        "test_data = [\n",
        "    \"Python is a programming language.\",\n",
        "    \"Machine learning involves training models on data.\",\n",
        "    \"Dogs are loyal pets.\",\n",
        "    \"Neural networks are inspired by the human brain.\"\n",
        "]\n",
        "\n",
        "test_vdb.addToDatabase(test_data)\n",
        "\n",
        "# Search for something\n",
        "query = \"What is ML?\"\n",
        "results, scores = test_vdb.search(query, n_return=2)\n",
        "\n",
        "print(f\"\\nðŸ” Query: '{query}'\\n\")\n",
        "for i, (chunk, score) in enumerate(zip(results, scores), 1):\n",
        "    print(f\"Result {i} (similarity: {score:.4f}):\")\n",
        "    print(f\"  {chunk}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fca1gqZN-q2b"
      },
      "source": [
        "## Step 4: PDF Processing Utilities\n",
        "\n",
        "Now we'll build functions to extract and process text from PDF files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMq2M-lg-q2b"
      },
      "source": [
        "### 4.1: Clean Text Function\n",
        "\n",
        "This function removes extra whitespace and newlines from text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "0SCHAtKB-q2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d55628c-5f8c-40b2-ff7d-5ffec75e6edc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: 'This   has    extra\\n\\nspaces   and\\nnewlines.'\n",
            "Cleaned:  'This has extra spaces and newlines.'\n"
          ]
        }
      ],
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Clean text by removing extra whitespace and newlines.\n",
        "\n",
        "    Args:\n",
        "        text: Raw text string\n",
        "\n",
        "    Returns:\n",
        "        Cleaned text string\n",
        "    \"\"\"\n",
        "    # Split text into words and join with single spaces\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "# Test the function\n",
        "test_text = \"This   has    extra\\n\\nspaces   and\\nnewlines.\"\n",
        "print(f\"Original: {repr(test_text)}\")\n",
        "print(f\"Cleaned:  {repr(clean_text(test_text))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1mOQqyO-q2b"
      },
      "source": [
        "### 4.2: Create Text Chunks\n",
        "\n",
        "**TODO**: Split long text into overlapping chunks.\n",
        "\n",
        "**Why Overlapping Chunks?**\n",
        "- Ensures we don't split important information across boundaries\n",
        "- Example: chunk_size=500, overlap=50 means each chunk shares 50 characters with the next"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "lNt53Nvi-q2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2804b57-5746-4f48-9ac8-8fe1ee357efa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text length: 100\n",
            "Number of chunks: 4\n",
            "First chunk length: 30\n"
          ]
        }
      ],
      "source": [
        "def chunksFromText(text: str, chunk_size=500, overlap=50):\n",
        "    \"\"\"\n",
        "    Split text into overlapping chunks.\n",
        "\n",
        "    Args:\n",
        "        text: Input text string\n",
        "        chunk_size: Size of each chunk in characters\n",
        "        overlap: Number of overlapping characters between chunks\n",
        "\n",
        "    Returns:\n",
        "        List of text chunks\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    # TODO: Calculate the step size (how much to move forward each time)\n",
        "    # Hint: step = chunk_size - overlap\n",
        "    step = chunk_size - overlap\n",
        "\n",
        "    # TODO: Loop through the text, creating chunks\n",
        "    # Hint: Use range(start, stop, step) where start=0, stop=len(text), step=calculated above\n",
        "    # For each position i, create a chunk from text[i:i+chunk_size]\n",
        "    for i in range(0, len(text), step):\n",
        "      chunks.append(text[i:i + chunk_size])\n",
        "    return chunks\n",
        "# Test the function\n",
        "test_text = \"A\" * 100  # 100 characters\n",
        "test_chunks = chunksFromText(test_text, chunk_size=30, overlap=5)\n",
        "print(f\"Text length: {len(test_text)}\")\n",
        "print(f\"Number of chunks: {len(test_chunks)}\")\n",
        "print(f\"First chunk length: {len(test_chunks[0]) if test_chunks else 0}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt8wflNB-q2b"
      },
      "source": [
        "### 4.3: Process PDF and Add to Database\n",
        "\n",
        "**TODO**: Read a PDF, extract text, create chunks, and add to the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "TrmUVi7N-q2b"
      },
      "outputs": [],
      "source": [
        "def chunksFromPDF(vDB, path: str, startPage=0, endPage=None):\n",
        "    \"\"\"\n",
        "    Extract text from a PDF, chunk it, and add to the vector database.\n",
        "\n",
        "    Args:\n",
        "        vDB: VectorDB instance to add chunks to\n",
        "        path: Path to the PDF file\n",
        "        startPage: First page to process (0-indexed)\n",
        "        endPage: Last page to process (None = all pages)\n",
        "    \"\"\"\n",
        "    # TODO: Create a PdfReader object\n",
        "    # Hint: reader = PdfReader(path)\n",
        "    reader = PdfReader(path)\n",
        "\n",
        "    # TODO: Get the list of pages to process\n",
        "    # Hint: Use reader.pages[startPage:endPage]\n",
        "    if endPage is None:\n",
        "      pages = reader.pages[startPage:]\n",
        "    else:\n",
        "      pages = reader.pages[startPage:endPage]\n",
        "\n",
        "    print(f\"ðŸ“„ Processing {len(pages)} pages from PDF...\")\n",
        "\n",
        "    all_chunks = []\n",
        "\n",
        "    # TODO: Loop through each page with tqdm for progress bar\n",
        "    for page_num, page in enumerate(tqdm(pages, desc=\"Extracting text\"), startPage):\n",
        "        # TODO: Extract text from the page\n",
        "        # Hint: Use page.extract_text()\n",
        "        text = page.extract_text()\n",
        "\n",
        "        # TODO: Clean the text\n",
        "        # Hint: Use the clean_text() function\n",
        "        text = clean_text(text)\n",
        "\n",
        "        # Skip empty or very short pages (likely covers or blank pages)\n",
        "        if len(text) < 100:\n",
        "            continue\n",
        "\n",
        "        # TODO: Convert text to chunks\n",
        "        # Hint: Use chunksFromText(text)\n",
        "        page_chunks = chunksFromText(text)\n",
        "\n",
        "        # Add page number to each chunk for reference\n",
        "        page_chunks = [f\"[Page {page_num+1}] {chunk}\" for chunk in page_chunks]\n",
        "        all_chunks.extend(page_chunks)\n",
        "\n",
        "    # TODO: Add all chunks to the vector database\n",
        "    # Hint: Use vDB.addToDatabase(all_chunks)\n",
        "    vDB.addToDatabase(all_chunks)\n",
        "\n",
        "    print(f\"âœ… Successfully processed PDF: {len(all_chunks)} chunks added to database\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwouMW2F-q2b"
      },
      "source": [
        "## Step 5: Question Answering System\n",
        "\n",
        "Now we'll create the function that ties everything together!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xatMiSWJ-q2b"
      },
      "source": [
        "### 5.1: Generate Answer Function\n",
        "\n",
        "**TODO**: Implement the RAG pipeline:\n",
        "1. Retrieve relevant context from the database\n",
        "2. Create a prompt with context and question\n",
        "3. Generate an answer using the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "OgazlYzi-q2c"
      },
      "outputs": [],
      "source": [
        "def generateAnswer(question: str, vDB, llm):\n",
        "    \"\"\"\n",
        "    Generate an answer to a question using RAG.\n",
        "\n",
        "    Args:\n",
        "        question: User's question\n",
        "        vDB: VectorDB instance with loaded documents\n",
        "        llm: Language model pipeline for generation\n",
        "\n",
        "    Returns:\n",
        "        Generated answer string\n",
        "    \"\"\"\n",
        "    # TODO: Search the database for relevant chunks\n",
        "    # Hint: Use vDB.search(question, n_return=3)\n",
        "    relevant_chunks, scores = vDB.search(question, n_return=3)\n",
        "\n",
        "    # TODO: Combine the chunks into a context string\n",
        "    # Hint: Use \"\\n\\n\".join(relevant_chunks)\n",
        "    context = \"\\n\\n\".join(relevant_chunks)\n",
        "\n",
        "    # TODO: Create a prompt that includes the context and question\n",
        "    # The prompt should ask the LLM to answer based on the context\n",
        "    prompt = f\"\"\"\n",
        "Based on the following context, answer the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"  # You can modify this prompt template\n",
        "\n",
        "    # TODO: Generate answer using the LLM\n",
        "    # Hint: result = llm(prompt, max_length=200, num_return_sequences=1)\n",
        "    result = llm(prompt, max_length=200, num_return_sequences=1)\n",
        "\n",
        "    # TODO: Extract the generated text from the result\n",
        "    # Hint: The result is a list of dictionaries with 'generated_text' key\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # TODO: Extract only the part after \"Answer:\"\n",
        "    # Hint: Use split(\"Answer:\")[-1].strip()\n",
        "    if \"Answer:\" in generated_text:\n",
        "      answer = generated_text.split(\"Answer:\")[-1].strip()\n",
        "    else:\n",
        "      answer = generated_text.strip()\n",
        "\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs0BJVGh-q2c"
      },
      "source": [
        "## Step 6: Put It All Together! ðŸŽ‰\n",
        "\n",
        "Now let's run the complete RAG chatbot!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_X5kYZ_-q2c"
      },
      "source": [
        "### 6.1: Login to Hugging Face (Optional)\n",
        "\n",
        "If you have an API key, this step helps avoid rate limits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "KpuKmyED-q2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eca66a7-93f6-4700-9a24-d8be634029eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging in to Hugging Face Hub...\n",
            "âš ï¸ Skipping login (API key optional): Invalid user token.\n"
          ]
        }
      ],
      "source": [
        "print(\"Logging in to Hugging Face Hub...\")\n",
        "try:\n",
        "    login(token=HF_API_KEY)\n",
        "    print(\"âœ… Login successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Skipping login (API key optional): {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmCA28RY-q2c"
      },
      "source": [
        "### 6.2: Initialize the Vector Database\n",
        "\n",
        "**TODO**: Create your VectorDB instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "LDljAFhM-q2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82c80b8c-ab5f-4758-ce95-a43774773651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embedding model...\n",
            "âœ… VectorDB initialized with embedding dimension: 384\n",
            "âœ… Vector Database ready!\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading embedding model...\")\n",
        "\n",
        "# TODO: Create an instance of VectorDB using EMBEDDING_MODEL\n",
        "# Hint: vDB = VectorDB(EMBEDDING_MODEL)\n",
        "vDB = VectorDB(EMBEDDING_MODEL)\n",
        "\n",
        "print(\"âœ… Vector Database ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xejdr6UO-q2c"
      },
      "source": [
        "### 6.3: Load and Process the PDF\n",
        "\n",
        "**TODO**: Process the TechNova IT Handbook PDF.\n",
        "\n",
        "**Note**: Make sure the PDF file is in the same directory as this notebook!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "3QFqibL8-q2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7864c5d1-ec8a-4adf-d26a-efd70093b902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“„ Found PDF: TechNova_IT_Handbook.pdf\n",
            "ðŸ“„ Processing 21 pages from PDF...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting text: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<00:00, 45.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Added 40 chunks. Total chunks: 40\n",
            "âœ… Successfully processed PDF: 40 chunks added to database\n"
          ]
        }
      ],
      "source": [
        "# Locate the PDF file\n",
        "pdf_path = \"TechNova_IT_Handbook.pdf\"\n",
        "\n",
        "# Check if file exists\n",
        "if not os.path.exists(pdf_path):\n",
        "    print(f\"âŒ PDF not found at {pdf_path}\")\n",
        "    print(\"Please make sure 'TechNova_IT_Handbook.pdf' is in the same folder as this notebook.\")\n",
        "else:\n",
        "    print(f\"ðŸ“„ Found PDF: {pdf_path}\")\n",
        "\n",
        "    # TODO: Call chunksFromPDF to process the PDF\n",
        "    # Hint: chunksFromPDF(vDB, pdf_path)\n",
        "    chunksFromPDF(vDB, pdf_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31W4oVsh-q2c"
      },
      "source": [
        "### 6.4: Load the Language Model\n",
        "\n",
        "**TODO**: Load the LLM for generating answers.\n",
        "\n",
        "**Note**: This may take a few minutes the first time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "UetrdVFd-q2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20aeb86c-8c17-474a-8dd7-395751bc50a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading LLM model...\n",
            "â³ This may take a few minutes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… LLM loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading LLM model...\")\n",
        "print(\"â³ This may take a few minutes...\")\n",
        "\n",
        "# TODO: Create a text generation pipeline\n",
        "# Hint: llm = pipeline(\"text2text-generation\", model=LLM_MODEL)\n",
        "llm = pipeline(\"text2text-generation\", model=LLM_MODEL)\n",
        "\n",
        "print(\"âœ… LLM loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btIkxQPv-q2c"
      },
      "source": [
        "### 6.5: Interactive Q&A Session\n",
        "\n",
        "**TODO**: Create an interactive loop for asking questions.\n",
        "\n",
        "Try asking questions like:\n",
        "- \"What is the password policy?\"\n",
        "- \"How do I report a security incident?\"\n",
        "- \"What are the remote work guidelines?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "A6JmRIqD-q2h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7704fe5-e72d-4f6e-a2d0-1dbff576c259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "ðŸ¤– RAGBot is ready!\n",
            "Ask questions about the TechNova IT Handbook.\n",
            "Type 'exit' or 'quit' to stop.\n",
            "==================================================\n",
            "\n",
            "\n",
            "â“ Your question: What is the password policy?\n",
            "\n",
            "ðŸ¤” Thinking...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ’¡ Answer: No reuse of last 10 passwords; rotate every 365 days.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "â“ Your question: How do I report a security incident?\n",
            "\n",
            "ðŸ¤” Thinking...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ’¡ Answer: Report to phishing@technova.com\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "â“ Your question: What are the remote work guidelines?\n",
            "\n",
            "ðŸ¤” Thinking...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ’¡ Answer: [iii]\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "â“ Your question: exit\n",
            "\n",
            "ðŸ‘‹ Goodbye! Thanks for using RAGBot!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ¤– RAGBot is ready!\")\n",
        "print(\"Ask questions about the TechNova IT Handbook.\")\n",
        "print(\"Type 'exit' or 'quit' to stop.\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "while True:\n",
        "    # Get user input\n",
        "    question = input(\"\\nâ“ Your question: \")\n",
        "\n",
        "    # Check if user wants to exit\n",
        "    if question.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"\\nðŸ‘‹ Goodbye! Thanks for using RAGBot!\")\n",
        "        break\n",
        "\n",
        "    # Skip empty questions\n",
        "    if not question.strip():\n",
        "        continue\n",
        "\n",
        "    # TODO: Generate and print the answer\n",
        "    # Hint: answer = generateAnswer(question, vDB, llm)\n",
        "    print(\"\\nðŸ¤” Thinking...\\n\")\n",
        "    answer = generateAnswer(question, vDB, llm)\n",
        "\n",
        "    print(f\"ðŸ’¡ Answer: {answer}\")\n",
        "    print(\"\\n\" + \"-\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI5cTcfL-q2h"
      },
      "source": [
        "### Alternative: Single Question Testing\n",
        "\n",
        "If you prefer to test with individual questions, use this cell instead:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "EuqPS3QT-q2h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "345510f2-915d-4813-acfa-c66941cb0771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the password policy?\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: No reuse of last 10 passwords; rotate every 365 days.\n"
          ]
        }
      ],
      "source": [
        "# Test with a single question\n",
        "test_question = \"What is the password policy?\"\n",
        "\n",
        "print(f\"Question: {test_question}\\n\")\n",
        "answer = generateAnswer(test_question, vDB, llm)\n",
        "print(f\"Answer: {answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OllyMZEt-q2h"
      },
      "source": [
        "## ðŸŽ“ Congratulations!\n",
        "\n",
        "You've successfully built a RAG chatbot! Here's what you accomplished:\n",
        "\n",
        "1. âœ… Created a vector database to store document embeddings\n",
        "2. âœ… Implemented similarity search using Euclidean distance\n",
        "3. âœ… Processed PDF documents and created text chunks\n",
        "4. âœ… Built a complete RAG pipeline for question answering\n",
        "5. âœ… Integrated an LLM to generate natural language responses\n",
        "\n",
        "### ðŸš€ Next Steps\n",
        "\n",
        "Want to improve your RAG bot? Try:\n",
        "- Experimenting with different chunk sizes and overlap values\n",
        "- Using cosine similarity instead of Euclidean distance\n",
        "- Adding more sophisticated text preprocessing\n",
        "- Trying different embedding models\n",
        "- Implementing a better prompt engineering strategy\n",
        "- Adding source citations to your answers\n",
        "\n",
        "### ðŸ“š Additional Resources\n",
        "\n",
        "- [SentenceTransformers Documentation](https://www.sbert.net/)\n",
        "- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)\n",
        "- [RAG Paper (Lewis et al.)](https://arxiv.org/abs/2005.11401)\n",
        "\n",
        "Great work! ðŸŽ‰"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}